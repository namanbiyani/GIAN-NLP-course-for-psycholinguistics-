{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIAN 9b: Building Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the data\n",
    "\n",
    "This step takes a long time. You can skip it and just import the data in *step 2*.\n",
    "\n",
    "We will use spaCy to tokenize the text, remove all the punctuation, and then save the data again in json format. In our new format, each piece of raw text will be replaced by a list of lemma's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from collections import *\n",
    "from itertools import *\n",
    "from math import log\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=en_core_web_sm.load(disable=[\"parser\",\"ner\"])\n",
    "def extract_lemmas(pipeline, s):\n",
    "    return([word.lemma_ for word in pipeline(s) if word.pos_ is not 'PUNCT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"language_log.json\", \"r\", encoding=\"utf-8\") as f_in:\n",
    "    posts=json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as progress\n",
    "\n",
    "for post in progress(posts, desc=\"Processed\"):\n",
    "    post['title']=extract_lemmas(nlp, post['title'])\n",
    "    post['entry']=extract_lemmas(nlp, post['entry'])\n",
    "    for comment in post.get('comments',[]):\n",
    "        comment['body']=extract_lemmas(nlp, comment['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"language_log_processed.json\", \"w\", encoding=\"utf-8\") as f_out:\n",
    "    json.dump(posts, f_out, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up simple networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install networkx if you haven't done so yet\n",
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's import our processed data again\n",
    "with open(\"language_log_processed.json\", \"r\", encoding=\"utf-8\") as f_in:\n",
    "    posts=json.load(f_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our first exercise, let's make a simple Graph where we connect commenters together when they comment on the same post.\n",
    "\n",
    "We will only use commenters who commented more than 100 times in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commenters=Counter([comment['author'] for post in posts for comment in post.get('comments',[])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_commenters=set([commenter for commenter,comment_frequency in commenters.items() if comment_frequency>=100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also build a list of pairs of commenters (commenters who commented on the same post at least once) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_commenters=set()\n",
    "for post in posts:\n",
    "    all_post_commenters=set([comment['author'] for comment in post.get('comments',[])])\n",
    "    frequent_post_commenters=all_post_commenters.intersection(frequent_commenters)\n",
    "    for c1,c2 in combinations(frequent_post_commenters, 2):\n",
    "        common_commenters.add((c1,c2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the graph of commenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.Graph()\n",
    "G.add_nodes_from(frequent_commenters)\n",
    "G.add_edges_from(common_commenters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can get some typical information from the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many commenters are in the network\n",
    "len(G.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many edges are in the network (links between commentors)\n",
    "len(G.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# how many other commenters is each commenter connected to\n",
    "G.degree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# who are the most connected commenters ?\n",
    "ranked_commenters=sorted(G.degree(), key=lambda x:-x[1])\n",
    "ranked_commenters[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# who are the least connected commenters ?\n",
    "ranked_commenters[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many components are there in the network, e.g. subnetworks that are not connected to eachother\n",
    "components=list(nx.connected_components(G))\n",
    "len(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the shortest path between the two least connected commenters?\n",
    "least_connected=([commenter_name for commenter_name, commenter_degree in ranked_commenters[-2:]])\n",
    "nx.shortest_path(G, *least_connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many different shortest paths are there between these commenters?\n",
    "len(list(nx.all_shortest_paths(G, *least_connected)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how close are commenters to other commenters on average?\n",
    "nx.closeness_centrality(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complex example. Graphs based on commenters' language use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def has_control_characters(word):\n",
    "    for character in word:\n",
    "        if unicodedata.category(character)[0]==\"C\":\n",
    "            result=True\n",
    "            break\n",
    "        else:\n",
    "            result=False\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commenter_words=defaultdict(list)\n",
    "for post in posts:\n",
    "    for comment in post.get(\"comments\", []):\n",
    "        commenter=comment['author']\n",
    "        if commenter in frequent_commenters:\n",
    "            for word in comment['body']:\n",
    "                if not has_control_characters(word):\n",
    "                    commenter_words[commenter].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the document frequency for every word\n",
    "# in this case, it is the number of commenters who use a word\n",
    "df={}\n",
    "for commenter, words in commenter_words.items():\n",
    "    for word in set(words):\n",
    "        df[word]=df.get(word,0)+1\n",
    "# now compute the inverse document frequency for every word\n",
    "\n",
    "n=len(frequent_commenters)\n",
    "idf={word: log(1+n/f) for word, f in df.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each commenter, find the 50 words with the highest ${TF} \\times {IDF}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niw=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciw={} # commenter informative words\n",
    "\n",
    "def highest(d, n):\n",
    "    \"\"\"Give the n highest scoring items in a dictionary\"\"\"\n",
    "    ds=sorted(d.items(), key=lambda x: -x[1])\n",
    "    return([item for item, value in ds][:n])\n",
    "\n",
    "for commenter, words in commenter_words.items():\n",
    "    ciw[commenter]=set(highest({word: log(f+1)*idf[word] for word, f in Counter(words).items()},niw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now construct a graph based on whether commenters have informative words in common\n",
    "G=nx.Graph()\n",
    "G.add_nodes_from(frequent_commenters)\n",
    "for commenter_a, commenter_b in combinations(frequent_commenters, 2):\n",
    "    if len(ciw[commenter_a].intersection(ciw[commenter_b]))>0:\n",
    "        G.add_edge(commenter_a, commenter_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([degree for commenter, degree in nx.degree(G)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the graph to a file in GraphML format\n",
    "networkx.write_graphml(G, open(\"commenters_informative_words.graphml\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the same graph, but now with weights on the edges\n",
    "G=nx.Graph()\n",
    "G.add_nodes_from(frequent_commenters)\n",
    "for commenter_a, commenter_b in combinations(frequent_commenters, 2):\n",
    "    weight=len(ciw[commenter_a].intersection(ciw[commenter_b]))/niw\n",
    "    G.add_edge(commenter_a, commenter_b, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the graph to a file in GraphML format\n",
    "networkx.write_graphml(G, open(\"commenters_informative_words_weighted.graphml\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a bipartite graph\n",
    "from networkx.algorithms import bipartite\n",
    "BG=nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commenter_nodes=[\"C: {:s}\".format(commenter) for commenter in frequent_commenters]\n",
    "BG.add_nodes_from(commenter_nodes, bipartite=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=set()\n",
    "for commenter,informative_words in ciw.items():\n",
    "    words.update(informative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.add_nodes_from(words, bipartite=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for commenter, informative_words in ciw.items():\n",
    "    for word in informative_words:\n",
    "        BG.add_edge(\"C: {:s}\".format(commenter), word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_bipartite(BG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PGwords=nx.bipartite.collaboration_weighted_projected_graph(BG, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkx.write_graphml(PGwords, open(\"pg_words.graphml\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PGcommenters=nx.bipartite.collaboration_weighted_projected_graph(BG, commenter_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkx.write_graphml(PGcommenters, open(\"pg_commenters.graphml\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
