{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# GIAN 3: Cleaning, Separating, and Counting\n",
    "\n",
    "You should now be familiar with searching and extracting text using regular expressions.\n",
    "\n",
    "This notebook shows how to clean text and split large chunks of text into smaller elements. It also shouws how you can count the elements of text and how you can use those counts directly to gain knowledge from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning\n",
    "\n",
    "After you have gathered your data, the next step in a text mining project is almost always to clean that data.\n",
    "\n",
    "Gathering and cleaning can be considered to be the preparatory work in text mining and are usually more than half of the total effort.\n",
    "\n",
    "In our current demonstration, we will have a relatively easy cleaning step.\n",
    "\n",
    "In other projects, more extensive cleaning steps may be involved, such as:\n",
    "\n",
    "+ Stripping html tags\n",
    "+ Removing time codes \n",
    "+ Removing duplicate documents\n",
    "+ Removing OCR errors\n",
    "+ ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we will use the file for Charles Dickens' \"[A Tale of Two Cities](https://www.gutenberg.org/ebooks/98)\", downloaded from [Project Gutenberg](https://www.gutenberg.org/). \n",
    "\n",
    "Books from project Gutenberg contain all kinds of legalese and licensing information before and after the main text. In many cases, we want to remove this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expressions are incredibly useful for cleaning!\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains the book as downloaded from Project Gutenberg\n",
    "t0 = open('GIAN3_data/pg98.txt', encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a Python [function](https://docs.python.org/3/tutorial/controlflow.html#defining-functions) to define some code that we can apply to any text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gutenberg_text(text):\n",
    "    \"\"\"Remove front and back matter from Gutenberg text\n",
    "    \n",
    "    Takes a text string corresponding to a project Gutenberg book \n",
    "    and returns a text string with front and back matter removed.\n",
    "    \"\"\"\n",
    "    m1 = re.search(\"START OF THIS PROJECT GUTENBERG EBOOK .+\\n\", text)\n",
    "    m2 = re.search(\"End of the Project Gutenberg EBook .+\\n\", text)\n",
    "    tstart=m1.span()[1]+1 # Text starts one character after the end of the front matter\n",
    "    tstop=m2.span()[0]  # Text ends one character before the beginning of the back matter \n",
    "    return(text[tstart:tstop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = clean_gutenberg_text(t0)\n",
    "len(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if we cleaned more or less correctly\n",
    "print(t1[:200]) # first 200 characters\n",
    "print(\"*******\")\n",
    "print(t1[-200:]) # final 200 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Separating or *tokenizing*\n",
    "\n",
    "In lecture 2, we have seen how we can build up regular expressions to split text into words. In the domain of NLP (Natural Language Processing), splitting a text into words is called *tokenizing*. Tokenization can become very complex, but luckily we can use existing NLP software to do it for us.\n",
    "\n",
    "For this lesson, we have chosen spaCy (https://spacy.io/), a recent library that tries to makes NLP practical and fast. SpaCy tries to give you the best available tools so you don't have to make any decision about which tokenizer, parser, or other component to use.\n",
    "\n",
    "If you want to install spaCy, follow the instructions on the [website](https://spacy.io/usage/).\n",
    "\n",
    "Things evolve quickly in NLP, so it is good to keep up-to-date about developments so that you can always use the best tool for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not installed an English language module for spacy, the code in the following cell provides an easy way to do so. If it doesn't work on your system, please [consult the documentation](https://spacy.io/usage/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the English language model and disable the parser and the tagger (because we only want to do tokenizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])\n",
    "nlp.max_length=2E6 # (makes spacy work with longer documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1=nlp(t1) # Tokenize the entire document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words are in the tokenized document ?\n",
    "nwords=len(doc1)\n",
    "print(nwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the text split into words, we can compute basic text statistics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lengths=[len(word) for word in doc1] # lengths of all the words\n",
    "mean_word_length=sum(word_lengths)/nwords\n",
    "min_word_length=min(word_lengths)\n",
    "max_word_length=max(word_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean word length:\", mean_word_length)\n",
    "print(\"Shortest word length:\", min_word_length)\n",
    "print(\"Longest word length:\", max_word_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even find words of a particular length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_of_length(words, length):\n",
    "    result=[word.lower_ for word in words if len(word)==length]\n",
    "    return(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"words\", words_of_length(doc1,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Counting words\n",
    "\n",
    "Word frequencies are the basic building blocks of many text mining techniques. Once text has been tokenized it becomes very easy to count how often each particular word occurs in that text.\n",
    "\n",
    "It is good to remember that:\n",
    "\n",
    "+ We call each particular word a word *type*\n",
    "+ We call every occurrence of a word a word *token*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_wf=Counter([word.orth_ for word in doc1]) # frequency of the word in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_wf.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make all words lowercase and get rid of punctuation!\n",
    "doc1_wf=Counter([word.lower_ for word in doc1 if re.search(\"\\w+\", word.lower_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_wf.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Frequencies\n",
    "It is very useful to have *relative frequencies* for words. It enables us to compare the frequencies of words across documents of different length.\n",
    "\n",
    "Typical scales for word frequencies are occurrences per million and per billion words.\n",
    "\n",
    "Transforming absolute frequencies to relative frequencies is very easy:\n",
    "\n",
    "$f_{rel}=\\frac{f_{abs}}{n}*s$\n",
    "\n",
    "In words: divide the absolute frequency $f_{abs}$ by the number of tokens $n$ in the document and multiply the result by the scale $s$ you want to use. So, if you want frequencies per million, you should multiply by 1000000, or, in scientific notation 1E6.\n",
    "\n",
    "For instance, if the frequency of \"the\" in our document is 8052 and the number of tokens in the document is 180841, then the relative frequency of \"the\" would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(8052/180841)*1E6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, we can do this for all words in a document at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=sum(doc1_wf.values())\n",
    "doc1_fpm=Counter({word: (frequency/n*1E6) for word, frequency in doc1_wf.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_fpm.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithmic Frequencies\n",
    "\n",
    "For NLP applications, frequencies are usually transformed to a logarithmic scale.\n",
    "\n",
    "For large documents, log10 of the frequency per billion words is a practical transformation.\n",
    "\n",
    "$log_{10}(\\frac{f_{abs}+1}{(n+{ntypes})}*s)$\n",
    "\n",
    "In other words: add one to the absolute frequency $f_{abs}$, divide this by sum of the number of tokens $n$ and the number of types $ntypes$ in the document, multiply this result by the scale $s$ you want to use, and, finally, take the log10 of this number. If we want frequencies per million words, $s$ will be 1E6; if we want frequencies per billion words, it will be 1E9.\n",
    "\n",
    "Another name for log10 frequencies per billion is the [Zipf scale](http://crr.ugent.be/archives/1352), named after the famous American linguist [George Kingsley Zipf](https://en.wikipedia.org/wiki/George_Kingsley_Zipf), who first described many of the properties of word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=sum(doc1_wf.values())\n",
    "ntypes=len(doc1_wf.values())\n",
    "doc1_zipf = Counter({word: log10((f+1)/(n+ntypes)*1E9) for word, f in doc1_wf.items()}) #log 10 frequency per billion words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_zipf.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The difference between frequencies and log frequencies\n",
    "\n",
    "Let's make a graph where the frequency of each word is plotted against its rank. The word with the highest frequency gets rank 1, the word with the second highest frequency gets rank 2, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies=sorted(doc1_fpm.values(), reverse=True)\n",
    "ranks=list(range(len(frequencies))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ranks, frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the curve is concentrated at the bottom edge of the graph: There are very few words with a high frequency. Almost all words have a frequency that is so low it's near the bottom.\n",
    "\n",
    "Log frequencies help us transform the frequencies, so that the difference between words with a frequency of 1 and 10 has the same importance as the difference between words with a frequency of 10 and 100, 100 and 1000, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the frequencies in descending order for the graph\n",
    "frequencies=sorted(doc1_zipf.values(),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(ranks, frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relation between a word's rank and its frequency is now much more visible on the graph!\n",
    "\n",
    "To make future processing easy, we wrote a function that takes a document that was tokenized by spaCy and returns log10 frequencies per billion words (zipf scale frequencies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipf(doc, alphanumeric=True, lowercase=True):\n",
    "    \"\"\"Compute log 10 frequencies per billion from a spaCy doc\"\"\"\n",
    "    if alphanumeric==True:\n",
    "        filter_pattern=\"\\w+\"\n",
    "    else:\n",
    "        filter_pattern=\".\"\n",
    "    if lowercase==True:\n",
    "        awf=Counter([word.lower_ for word in doc if re.search(filter_pattern, word.lower_)])\n",
    "    else:\n",
    "        awf=Counter([word.orth_ for word in doc if re.search(filter_pattern, word.orth_)])\n",
    "    n=sum(awf.values())\n",
    "    ntypes=len(awf.values())\n",
    "    zipf = Counter({word: log10((f+1)/(n+ntypes)*1E9) for word, f in awf.items()})\n",
    "    return(zipf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. An application: Language identification\n",
    "\n",
    "Word frequencies give us a convenient way to identify the language of a document.\n",
    "\n",
    "Let's take some text in another language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = open('GIAN3_data/pg22367.txt', encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove front and back matter\n",
    "t2=clean_gutenberg_text(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the content (we can use the English language tokenizer for most alphabetic languages)\n",
    "doc2=nlp(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2_zipf=zipf(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2_zipf.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare this with the 10 most frequent words from the earlier document\n",
    "doc1_zipf.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the top words for documents in a particular language are always very similar to each other ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(clean_gutenberg_text(open('GIAN3_data/pg5200.txt', encoding=\"utf-8\").read()))\n",
    "doc3_zipf=zipf(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3_zipf.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(clean_gutenberg_text(open('GIAN3_data/40739-0.txt', encoding=\"utf-8\").read()))\n",
    "doc4_zipf = zipf(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4_zipf.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build an easy German-English text classification system by:\n",
    "\n",
    "- Taking a number of documents for which we know the language\n",
    "- Comparing the top $n$ words for a new document to the top $n$ words in each of those documents\n",
    "- Predicting the language of the new document from the language of the document with the highest overlap in the top $n$ words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docbase=((doc1_zipf, \"English\"), (doc2_zipf, \"German\"), (doc3_zipf, \"English\"), (doc4_zipf, \"German\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def german_or_english(newdoc, docbase, n=10):\n",
    "    \"\"\"Predict whether a document is German or English\"\"\"\n",
    "    newdoc_zipf=zipf(newdoc)\n",
    "    results=[]\n",
    "    for doc_zipf, language in docbase:\n",
    "        new_topwords=[word for word, frequency in newdoc_zipf.most_common(n)]\n",
    "        db_topwords=[word for word, frequency in doc_zipf.most_common(n)]\n",
    "        topword_overlap=len(set(new_topwords).intersection(db_topwords))\n",
    "        results.append((topword_overlap, language))\n",
    "    results.sort(reverse=True)\n",
    "    return(results[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc5=nlp(clean_gutenberg_text(open('GIAN3_data/pg26971.txt', encoding=\"utf-8\").read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_or_english(doc5, docbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc5[1000:1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc6=nlp(clean_gutenberg_text(open('GIAN3_data/pg27000.txt', encoding=\"utf-8\").read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_or_english(doc6, docbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc6[600:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t7= \"\"\"I want to live,\n",
    "I want to give\n",
    "I've been a miner\n",
    "for a heart of gold\n",
    "It's these expressions\n",
    "I never give\n",
    "That keep me searching\n",
    "for a heart of gold\n",
    "And I'm getting old\n",
    "Keeps me searching\n",
    "for a heart of gold\n",
    "And I'm getting old\n",
    "\n",
    "I've been to Hollywood\n",
    "I've been to Redwood\n",
    "I crossed the ocean\n",
    "for a heart of gold\n",
    "I've been in my mind,\n",
    "it's such a fine line\n",
    "That keeps me searching\n",
    "for a heart of gold\n",
    "And I'm getting old\n",
    "Keeps me searching\n",
    "for a heart of gold\n",
    "And I'm getting old\n",
    "\n",
    "Keep me searching\n",
    "for a heart of gold\n",
    "You keep me searching\n",
    "And I'm growing old\n",
    "Keep me searching\n",
    "for a heart of gold\n",
    "I've been a miner\n",
    "for a heart of gold\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc7=nlp(t7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_or_english(doc7, docbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_or_english(nlp(\"the cricket and the ant\"), docbase) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
