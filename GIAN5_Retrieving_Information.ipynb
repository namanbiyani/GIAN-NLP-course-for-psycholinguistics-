{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIAN 5: Retrieving Information\n",
    "\n",
    "You should now be familiar with how natural language processing techniques such as parsing, part-of-speech tagging, lemmatizing, and named-entity recognition can be used in text mining.\n",
    "\n",
    "In this notebook you will learn basic techniques for identifying informative words in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the libraries required for this session\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from math import *\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and define some elementary functions\n",
    "def highest(d, n):\n",
    "    \"\"\"Give the n highest scoring items in a dictionary\"\"\"\n",
    "    ds=sorted(d.items(), key=lambda x: -x[1])\n",
    "    return([item for item, value in ds][:n])\n",
    "\n",
    "def lowest(d, n):\n",
    "    \"\"\"Give the n lowest scoring items in a dictionary\"\"\"\n",
    "    ds=sorted(d.items(), key=lambda x: x[1])\n",
    "    return([item for item, value in ds][:n])\n",
    "\n",
    "def print_doc_info(doc, searchwords, nsentences=5):\n",
    "    doc_sentences=[]\n",
    "    doc=nlp(doc.text)\n",
    "    print(\"\\n****************\")\n",
    "    print(doc_names[docnum])\n",
    "    print(\"\\nMOST IMPORTANT NAMED ENTITIES:\"),\n",
    "    print(', '.join(highest(Counter(ent.orth_ for ent in doc.ents),10)))\n",
    "    print(\"\\nMATCHING SENTENCES:\"),\n",
    "    j=0\n",
    "    for sent in doc.sents:\n",
    "        if any([token.lower_ in searchwords for token in sent]):\n",
    "            print(sent)\n",
    "            j=j+1\n",
    "        if j>=nsentences:\n",
    "            break\n",
    "    print(\"****************\\n\")\n",
    "\n",
    "def best_matches(searchbase, searchwords, n, match_all = True):\n",
    "    \"\"\"Find the n documents where search words are ranked most highly.\n",
    "    \n",
    "    Allows for finding any or all search terms with match_all=\"\"\"\n",
    "    scores=[]\n",
    "    for docnum, docwords in searchbase:\n",
    "        matches=[searchword in docwords for searchword in searchwords]\n",
    "        if (match_all==True and all(matches)) or (match_all==False and any(matches)):\n",
    "            match_ranks=[docwords.index(searchword) for i,searchword in enumerate(searchwords) if matches[i]==True]\n",
    "            scores.append((docnum, sum(match_ranks)))\n",
    "    return(sorted(scores)[:n])\n",
    "    \n",
    "def find_similar(searchbase, docnum, n):\n",
    "    \"\"\"Find n documents with the highest overlap in keywords\"\"\"\n",
    "    target_words=set(searchbase[docnum][1])\n",
    "    results=[]\n",
    "    for i, docwords in searchbase:\n",
    "        if docnum==i:\n",
    "            continue\n",
    "        matchwords=target_words.intersection(docwords)\n",
    "        r=len(matchwords)\n",
    "        results.append((i, r, matchwords))\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return(results[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy NLP pipeline\n",
    "# which resource you load here is dependent on the models you installed\n",
    "# see https://spacy.io/docs/usage/models\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load(disable=[\"parse\", \"tag\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using Corpora\n",
    "\n",
    "In this lesson, we will start working with collections of documents, or, in other words, corpora. \n",
    "\n",
    "We will use a sample of the BBC subtitle corpus collected by [Van Heuven, Mandera, Keuleers, & Brysbaert (2014)](http://www.tandfonline.com/doi/abs/10.1080/17470218.2013.850521)\n",
    "\n",
    "The files have already been cleaned of time stamps, so they are ready to be processed by spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir=\"GIAN5_data\"\n",
    "filenames=[filename for filename in os.listdir(corpus_dir) if filename.endswith('srt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_filenames=random.sample(filenames, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run spaCy's NLP pipeline to tokenize all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs=[]\n",
    "doc_names=[]\n",
    "for filename in my_filenames:\n",
    "    f=open(\"{:s}/{:s}\".format(corpus_dir,filename), encoding=\"utf-8\")\n",
    "    s=f.read()\n",
    "    if len(s)>100:   # process only files with more than 100 characters\n",
    "        docs.append(nlp(s))\n",
    "        doc_names.append(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document length, vocabulary, and the type to token ratio\n",
    "\n",
    "When we start out analyzing a corpus, a first thing to learn is the length of the documents in the corpus.\n",
    "\n",
    "Let us start out by finding out the length of each document and making a histogram of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tokens=[[token.lower_ for token in doc if token.is_alpha] for doc in docs]\n",
    "docs_ntokens=[len(doc) for doc in docs_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(docs_ntokens)\n",
    "plt.xlabel(\"Number of tokens\")\n",
    "plt.ylabel(\"Number of Documents\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the histogram shows a concentration of values on the left and a long tail on the right: our corpus is composed of mostly of short documents and very long documents are very rare. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let us now move from the number of tokens to the number of types in each document. \n",
    "\n",
    "Another way to look at number of types is as the *vocabulary* of the document. The more different words are used, the larger the document's vocabulary. \n",
    "\n",
    "As we have learned in previous lectures, we can easily build a frequency dictionary of a document. The number of entries in that frequency dictionary gives us the number of types in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make a list of frequency dictionaries\n",
    "docs_fds=[Counter(doc) for doc in docs_tokens]\n",
    "# Then, get the length of each frequency dictionary\n",
    "docs_ntypes=[len(fd) for fd in docs_fds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And plot the histogram\n",
    "plt.hist(docs_ntypes)\n",
    "plt.xlabel(\"Number of Types\")\n",
    "plt.ylabel(\"Number of Documents\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is likely that longer documents also have more types. But if this were a perfect relation, the histograms above would be identical.\n",
    "\n",
    "Let's make a scatterplot to gain more insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(docs_ntokens, docs_ntypes)\n",
    "plt.ylim(0)\n",
    "plt.xlim(0)\n",
    "plt.xlabel(\"Number of tokens\")\n",
    "plt.ylabel(\"Number of types\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There clearly is a very strong relation! But still there are documents of widely different length that have about the same number of types. This means that in some documents more words are used repeatedly than in other documents. We could say that the former documents have a poorer vocabulary, while documents with more types for the same amount of tokens have a richer vocabulary\n",
    "\n",
    "$TTR=\\frac{ntypes}{ntokens}$\n",
    "\n",
    "It is easy to see that the ratio between number of types and number of tokens expresses something about the variety of words used in that document: the higher the ratio, the richer the vocabulary.\n",
    "\n",
    "Let's compute the TTR for each document and plot it against the number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_ttrs=[docs_ntypes[i]/docs_ntokens[i] for i in range(len(docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(docs_ntokens, docs_ttrs)\n",
    "plt.ylim(0)\n",
    "plt.xlim(0)\n",
    "plt.xlabel(\"Number of tokens\")\n",
    "plt.ylabel(\"TTR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, in general, longer documents have a smaller type to token ratio than shorter documents. We also see that the TTR expresses a lot of variation. For documents that are the same length, the TTR adequately expresses the difference in vocabulary.\n",
    "\n",
    "Looking at the graph, we could now choose some documents that we are interested in comparing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finding informative words with ${TF} \\times {IDF}$\n",
    "\n",
    "As we have seen, frequency dictionaries tell us which words occur frequently in a document. However, we have also seen that the most frequent words in a document are not very informative.\n",
    "\n",
    "Let's take the top 10 words of the first document in our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_fds[0].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could be *any* document. However, this insight also gives us a very important clue: If we want to identify words which are important in a document, we need to identify the words which have a high occurence specifically in that document and not in other documents.\n",
    "\n",
    "So let's first look at words which are not informative by looking at how many documents each word occurs in. If a word is not informative, it will tend to occur in many documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=Counter()\n",
    "for doc_fd in docs_fds:\n",
    "    df.update(set(doc_fd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some common words\n",
    "df.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest(df, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final formula to work we need to transform this document frequency as follows:\n",
    "\n",
    "For each word divide the total number of documents by the document frequency and take the logarithm of that fraction.\n",
    "\n",
    "This is also known as the inverse document frequency. It will be lower when a word occurs in more documents.\n",
    "\n",
    "${IDF}=log(1+\\frac{ndocs}{ndocs_{word}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(docs)\n",
    "idf={word: log(1+n/f) for word, f in df.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest(idf,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to transform our raw frequencies so that they become log scaled. \n",
    "\n",
    "This is called term frequency or $tf$.\n",
    "\n",
    "${TF}=log(1+f_{word})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tfs=[]\n",
    "for doc_fd in docs_fds:\n",
    "    doc_tf={word:log(1+f) for word, f in doc_fd.items()}\n",
    "    docs_tfs.append(doc_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can multiply $TF$ and $IDF$ for all the words in all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tfidfs=[]\n",
    "for doc_tf in docs_tfs:\n",
    "    doc_tfidf={word: tf*idf[word] for word, tf in doc_tf.items()}\n",
    "    docs_tfidfs.append(doc_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can find the words with the best ${TF}\\times{IDF}$  in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docnum=99\n",
    "doc=docs_tfidfs[docnum]\n",
    "docname=doc_names[docnum]\n",
    "print(highest(doc, 20))\n",
    "print(docname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finding documents in which particular words are informative\n",
    "\n",
    "Now that we have computed ${TF}\\times{IDF}$ scores, we can also define words that we are particularly interested in and rank the documents according to the score they give for that word.\n",
    "\n",
    "Note that using pure term frequency  would give exactly the same result as ${TF}\\times{IDF}$ when we have all the word frequencies at our disposal. However, with ${TF}\\times{IDF}$, we can simply characterize documents by their most informative words and throw away the information about all the other words. This makes searching much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a search base containing each document's number and its n most informative words\n",
    "n=200\n",
    "my_searchbase = [(i, highest(docs_tfidfs[i], 200)) for i in range(len(docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_searchbase[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchwords=[\"chicken\"]\n",
    "matchlist=best_matches(my_searchbase, searchwords, 5, match_all=True)\n",
    "matchlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_names[128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You can change parameters in the cell above: use more searchwords and see what the effect is of setting match_all to False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found documents but we still don't know why the documents are interesting. \n",
    "\n",
    "We can start by looking at the other words with a high ${TF}\\times{IDF}$ in the documents we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for docnum, rank in matchlist:\n",
    "    print(my_searchbase[docnum][1][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with a little more effort we can find out more about the document that matches our search.\n",
    "\n",
    "Here, we output the top named entities and the sentences that the searchword occurs in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for docnum, rank in matchlist:\n",
    "    print_doc_info(docs[docnum], searchwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Finding similar documents\n",
    "\n",
    "An interesting application of corpora is finding similar documents. There are many different ways to do this.\n",
    "\n",
    "Here, we illustrate a particularly simple method.\n",
    "\n",
    "Given our search base in which each document is listed with its top 50 keywords, we can focus on one document and find out which other documents share most of the same keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_docnum=matchlist[1][0]\n",
    "similar_documents=find_similar(my_searchbase, my_docnum, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for docnum, rank, docwords in similar_documents:\n",
    "    print(\"DOCUMENT:\", docnum, \"; MATCHING KEYWORDS:\", ', '.join(docwords))\n",
    "    print_doc_info(docs[docnum], searchwords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
