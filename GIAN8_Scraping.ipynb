{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIAN 7: Scraping\n",
    "\n",
    "The internet provides a great source of data that can be used for research in language processing. \n",
    "\n",
    "We are confronted with two problems.\n",
    "\n",
    "1. How can we get automatically download content from the internet\n",
    "2. How can we parse downloaded webpages to extract exactly what we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Automatically downloading content\n",
    "\n",
    "Most popular websites prevent downloading by bots. Repeated access to websites is usally quickly blocked. The key to downloading content for scientific research from such websites is to do it in moderation and using your own webbrowser.\n",
    "\n",
    "One of the ways in which you can automate your browser is [Selenium](https://www.seleniumhq.org/). In this tutorial, I will assume that you use Selenium in combination with the [Firefox](https://www.mozilla.org/en-US/firefox/download/) web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Selenium\n",
    "\n",
    "If you are using the Anaconda Navigator, you can install the Selenium package for Python from there. Alternatively, you can install it by running the command in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to install a driver to communicate with your browser. The drivers for Firefox can be downloaded from https://github.com/mozilla/geckodriver/releases\n",
    "\n",
    "Please note that you will need to install the driver in a directory on your *path*. Running the cell below shoul you the directories on your path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commands in the following cell:\n",
    "+ downloads the current driver for Firefox (December 13, 2018) to the current directory (the same as this notebook is located in);\n",
    "+ unpacks the driver if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## uncomment the following lines if you use macOS\n",
    "# driver_url = \"https://github.com/mozilla/geckodriver/releases/download/v0.23.0/geckodriver-v0.23.0-macos.tar.gz\"\n",
    "# driver_filename = driver_url.split(\"/\")[-1]\n",
    "# urllib.request.urlretrieve(driver_url, driver_filename)\n",
    "# archive = tarfile.open(driver_filename)\n",
    "# archive.extractall\n",
    "# archive.close()\n",
    "\n",
    "## uncomment the following lines if you use Windows\n",
    "# driver_url = \"https://github.com/mozilla/geckodriver/releases/download/v0.23.0/geckodriver-v0.23.0-win64.zip\"\n",
    "# driver_filename = driver_url.split(\"/\")[-1]\n",
    "# urllib.request.urlretrieve(driver_url, driver_filename)\n",
    "# archive = zipfile.ZipFile(driver_filename)\n",
    "# archive.extractall()\n",
    "# archive.close()\n",
    "\n",
    "## uncomment the following lines if you use Linux\n",
    "# driver_url = \"https://github.com/mozilla/geckodriver/releases/download/v0.23.0/geckodriver-v0.23.0-linux64.tar.gz\"\n",
    "# driver_filename = driver_url.split(\"/\")[-1]\n",
    "# urllib.request.urlretrieve(driver_url, driver_filename)\n",
    "# archive = tarfile.open(driver_filename)\n",
    "# archive.extractall()\n",
    "# archive.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will need to manually copy the driver you downloaded to a location in your path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Selenium\n",
    "\n",
    "We are now ready to run Selenium. We will use the [documentation for the Python Selenium API](https://selenium-python.readthedocs.io/) to learn how to use Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, the following code should launch a Firefox instance and open the frontpage of Language Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to scrap the website Language Log, which can be found at [http://languagelog.ldc.upenn.edu](). However, we don't want to overload the site, so we have placed a local copy at [http://172.20.160.50/user/sikarwar/gyan/languagelog.ldc.upenn.edu]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url=\"http://172.20.160.50/user/sikarwar/gyan/languagelog.ldc.upenn.edu/index.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get(base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write the page we scraped to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"language_log.html\", \"w\", encoding=\"utf-8\") as f_out:\n",
    "    f_out.write(browser.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use selenium to control the browser.\n",
    "\n",
    "Let's try to expand the link showing the archives of the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_element_by_link_text(\"[+/–]\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And go into the archives for December 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_element_by_link_text(\"December 2018\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command will stop the driver and quit the browser instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to load the website and control the browser, let's try something more complicated. We will download a number of pages from the Language Log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first make a directory to store our downloaded files and create a text file that logs what we have downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hashlib import md5\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(\"loot\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "# create a log file\n",
    "download_log=open(\"loot/log_pages.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also write a simple function that takes a page source, creates a unique filename for the file and stores the information containing the filename and its URL in the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_page(browser, logfile):\n",
    "    # create a unique filename\n",
    "    source_hash=md5(browser.page_source.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    # get the required information from the page\n",
    "    current_filename=\"loot/{:s}.html\".format(source_hash)\n",
    "    current_url=browser.current_url\n",
    "    current_source=browser.page_source\n",
    "\n",
    "    # store the page source in the file\n",
    "    with open(current_filename, \"w\") as f_out:\n",
    "        f_out.write(current_source)\n",
    "\n",
    "    # store the filename and url in the download log\n",
    "    logfile.write(\"{:s}\\t{:s}\\n\".format(current_url, current_filename))\n",
    "    logfile.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start our browser again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Firefox()\n",
    "browser.get(base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And go to the archives for November 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_element_by_link_text(\"[+/–]\").click()\n",
    "browser.find_element_by_link_text(\"November 2018\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For safety, we always start by testing on a few pages\n",
    "for i in range(5):\n",
    "    store_page(browser, download_log)\n",
    "    try:\n",
    "        browser.find_element_by_link_text(\"Next Page »\").click()\n",
    "    except:\n",
    "        break\n",
    "    time.sleep(random.randint(5,10)) # wait for 5 to 10 seconds  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()\n",
    "download_log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the structure of the url's, we could think that there is a more efficient way of downloading all the posts.\n",
    "\n",
    "Every url for a post is of the form `http://languagelog.ldc.upenn.edu/nll/?p=X`, where X is the number of the post\n",
    "\n",
    "We could generate all the url's and downloads all the corresponding posts!\n",
    "\n",
    "However, the posts are not numbered sequentially and a browser generating many 404s could quickly be identifed as a bot.\n",
    "\n",
    "It's better to take things slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parsing web content\n",
    "When we download web pages, we invariably download them in html format. Once we know how to parse the html structure, it can actually be very useful for extracting exactly the information we want.\n",
    "\n",
    "In this case, we want to make a list of the url's of the *posts* on Language Log.\n",
    "\n",
    "Let's look at our log file to see the files we have downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"loot/log_pages.txt\", \"r\", encoding=\"utf-8\") as logfile:\n",
    "    filenames=[]\n",
    "    for line in logfile:\n",
    "        url, filename = line.strip().split(\"\\t\")\n",
    "        filenames.append(filename)\n",
    "print(len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BeautifulSoup\n",
    "\n",
    "[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is a library that parses a webpage's [Document Object Model](https://www.w3.org/TR/WD-DOM/introduction.html) and lets us navigate that structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install beautifulsoup using the following command or via Anaconda\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file=open(filenames[0])\n",
    "soup = BeautifulSoup(test_file, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspection of the source of one of the pages, shows that a link to a post has this form.\n",
    "\n",
    "`<h2 class=\"posttitle\" id=\"post-38242\"><a href=\"http://languagelog.ldc.upenn.edu/nll/?p=38242\" rel=\"bookmark\" title=\"Permanent link to Smart should check the OED\">Smart should check the OED</a></h2>`\n",
    "\n",
    "Using BeautifulSoup, we can search for all `h2` elements with class `posttitle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"h2\", \"posttitle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from each of those we can extract the link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h2 in soup.find_all(\"h2\", \"posttitle\"):\n",
    "    print(h2.a.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do this for all the pages we downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_urls=[]\n",
    "for filename in filenames:\n",
    "    soup = BeautifulSoup(open(filename), 'html.parser')\n",
    "    for h2 in soup.find_all(\"h2\", \"posttitle\"):\n",
    "        post_url=h2.a.get(\"href\")\n",
    "        post_urls.append(post_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(post_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_urls=[\"http://languagelog.ldc.upenn.edu/nll/?p=690\", \"http://languagelog.ldc.upenn.edu/nll/?p=689\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=686\", \"http://languagelog.ldc.upenn.edu/nll/?p=685\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=681\", \"http://languagelog.ldc.upenn.edu/nll/?p=684\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=683\", \"http://languagelog.ldc.upenn.edu/nll/?p=682\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=677\", \"http://languagelog.ldc.upenn.edu/nll/?p=675\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=674\", \"http://languagelog.ldc.upenn.edu/nll/?p=673\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=672\", \"http://languagelog.ldc.upenn.edu/nll/?p=670\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=671\", \"http://languagelog.ldc.upenn.edu/nll/?p=669\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=666\", \"http://languagelog.ldc.upenn.edu/nll/?p=668\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=664\", \"http://languagelog.ldc.upenn.edu/nll/?p=665\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=663\", \"http://languagelog.ldc.upenn.edu/nll/?p=662\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=661\", \"http://languagelog.ldc.upenn.edu/nll/?p=660\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=657\", \"http://languagelog.ldc.upenn.edu/nll/?p=659\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=658\", \"http://languagelog.ldc.upenn.edu/nll/?p=656\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=655\", \"http://languagelog.ldc.upenn.edu/nll/?p=654\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=653\", \"http://languagelog.ldc.upenn.edu/nll/?p=652\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=651\", \"http://languagelog.ldc.upenn.edu/nll/?p=649\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=650\", \"http://languagelog.ldc.upenn.edu/nll/?p=648\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=647\", \"http://languagelog.ldc.upenn.edu/nll/?p=645\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=646\", \"http://languagelog.ldc.upenn.edu/nll/?p=644\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=643\", \"http://languagelog.ldc.upenn.edu/nll/?p=642\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=641\", \"http://languagelog.ldc.upenn.edu/nll/?p=640\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=639\", \"http://languagelog.ldc.upenn.edu/nll/?p=638\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=636\", \"http://languagelog.ldc.upenn.edu/nll/?p=635\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=637\", \"http://languagelog.ldc.upenn.edu/nll/?p=634\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=633\", \"http://languagelog.ldc.upenn.edu/nll/?p=632\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=631\", \"http://languagelog.ldc.upenn.edu/nll/?p=630\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=629\", \"http://languagelog.ldc.upenn.edu/nll/?p=628\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=627\", \"http://languagelog.ldc.upenn.edu/nll/?p=626\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=625\", \"http://languagelog.ldc.upenn.edu/nll/?p=624\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=623\", \"http://languagelog.ldc.upenn.edu/nll/?p=622\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=620\", \"http://languagelog.ldc.upenn.edu/nll/?p=619\",\n",
    "\"http://languagelog.ldc.upenn.edu/nll/?p=615\"]\n",
    "\n",
    "post_urls=error_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use Selenium to download the actual posts.\n",
    "\n",
    "Again, we'll first make a log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_log=open(\"loot/log_posts.txt\", \"a\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Firefox()\n",
    "for post_url in post_urls:\n",
    "    browser.get(post_url)\n",
    "#     time.sleep(random.randint(5,10))\n",
    "    store_page(browser, download_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()\n",
    "download_log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have downloaded the posts, we can parse them with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict # used to make it easier to use dictionaries\n",
    "from collections import Counter     # makes it easier to build frequency lists\n",
    "from datetime import datetime       # convert timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./loot/log_posts.txt\", \"r\", encoding=\"utf-8\") as logfile:\n",
    "    filenames=[]\n",
    "    for line in logfile:\n",
    "        url, filename = line.strip().split(\"\\t\")\n",
    "        filenames.append(filename)\n",
    "print(len(filenames))\n",
    "test_file=open(filenames[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(test_file, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write small functions to extract the different components of the posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_post_title(bs):\n",
    "    return(soup.find(\"h2\", \"posttitle\").a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_post_title(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_post_meta(bs):\n",
    "    d=defaultdict(list)\n",
    "    timestamp_text=bs.find(\"p\", \"postmeta\").text.strip().split(\"\\n\")[0].strip()\n",
    "    timestamp=datetime.strptime(timestamp_text, \"%B %d, %Y @ %I:%M %p\")\n",
    "    d['timestamp']=str(timestamp)\n",
    "    for metalink in bs.find(\"p\", \"postmeta\").find_all(\"a\"):\n",
    "        key=metalink.get(\"rel\")[0]\n",
    "        value=metalink.text\n",
    "        d[key].append(value)\n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_post_meta(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_post_entry(bs):\n",
    "    raw_paragraphs=[]\n",
    "    for paragraph in (bs.find(\"div\", \"postentry\").find_all(\"p\")[1:]):\n",
    "        if paragraph.get(\"class\")==[\"postmeta\"]:\n",
    "            break\n",
    "        else:\n",
    "            raw_paragraphs.append(paragraph.text)\n",
    "    return(\"\\n\".join(raw_paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_post_entry(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comments(bs):\n",
    "    comments=[]\n",
    "    comment_section=bs.find(id=\"commentlist\")\n",
    "    if comment_section:\n",
    "        for comment_li in comment_section.find_all('li'):\n",
    "            comment={}\n",
    "            author=comment_li.find(\"h3\",\"commenttitle\").text[:-6]\n",
    "            comment[\"author\"]=author\n",
    "            timestamp_text=comment_li.find(\"p\", \"commentmeta\").text.strip()\n",
    "            timestamp=datetime.strptime(timestamp_text, \"%B %d, %Y @ %I:%M %p\")\n",
    "            comment['timestamp']=str(timestamp)\n",
    "            body=\"\".join([paragraph.text for paragraph in comment_li.find_all(\"p\", class_=False)])\n",
    "            comment['body']=body\n",
    "            comments.append(comment)\n",
    "    return(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_comments(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract the information from all of the posts. As always, start with a few posts to test our functionality. If everything works well, you can process all your posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts=[]\n",
    "processing_counter=0\n",
    "for filename in filenames:\n",
    "    try:\n",
    "        post={}\n",
    "        soup = BeautifulSoup(open(filename), 'html.parser')\n",
    "        post['title']=extract_post_title(soup)\n",
    "        post.update(extract_post_meta(soup))\n",
    "        post['entry']=extract_post_entry(soup)\n",
    "        posts.append(post)\n",
    "        post['comments']=extract_comments(soup)\n",
    "    except:\n",
    "        print(\"procedure did not work for post {:d}: {:s}\".format(processing_counter, filename))\n",
    "    processing_counter=processing_counter+1\n",
    "    if processing_counter%100==0:\n",
    "        print(\"processed {:d} posts\".format(processing_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the data we need, let's export it.\n",
    "\n",
    "We will use the *json* format to store the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"language_log.json\", \"w\", encoding=\"utf-8\") as f_out:\n",
    "    json.dump(posts, f_out, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
