{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIAN 4: Resources and Recognition\n",
    "\n",
    "You should now be familiar with tokenizing text and using frequency measures.\n",
    "\n",
    "This notebook will show you how to use annotated text resources for tokenizing, lemmatizing, part-of-speech tagging, parsing, and named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the libraries required for this session\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import spacy\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell the notebook that we want plots to be displayed inside of the notebook (don't open a new window)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define some essential functions for this session\n",
    "\n",
    "def clean_gutenberg_text(text):\n",
    "    \"\"\"Remove front and back matter from project Gutenberg texts\"\"\"\n",
    "    m1 = re.search(\"START OF THIS PROJECT GUTENBERG EBOOK .+\\n\", text)\n",
    "    m2 = re.search(\"End of the Project Gutenberg EBook .+\\n\", text)\n",
    "    tstart=m1.span()[1]+1 # Text starts one character after the end of the front matter\n",
    "    tstop=m2.span()[0]  # Text ends one character before the beginning of the back matter\n",
    "    # spaCy's tokenizer doesn't like newlines\n",
    "    text=re.sub(\"\\n+\", \"\\n\", text[tstart:tstop])\n",
    "    return(text)\n",
    "\n",
    "def freq_over_time(doc, tracklist):\n",
    "    \"\"\"Track the frequency of named entities over the course of a document.\n",
    "    \n",
    "    Returns a dictionary (trackdict) where keys correspond to the entities from the tracklist,\n",
    "    and values correspond to a list with all the token positions for the corresponding entity.\"\"\"\n",
    "    trackdict={entity:[0] for entity in tracklist}\n",
    "    for ent in doc.ents:\n",
    "        for entity in tracklist:\n",
    "            last_value=trackdict[entity][-1]\n",
    "            if (ent.orth_, ent.label_)==entity:\n",
    "                trackdict[entity].append(last_value+1)\n",
    "            else:\n",
    "                trackdict[entity].append(last_value)\n",
    "    return trackdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the full NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load spaCy resources for English\n",
    "# see https://spacy.io/docs/usage/models\n",
    "\n",
    "# To avoid problems with unlinked modules,\n",
    "# we use an alternative import syntax\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we haven't disabled any modules, because for this lecture, in addition to tokenizing, we also will do parsing, part-of-speech tagging, and named entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the Project Gutenberg file for \"A Tale of Two Cities\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = open('GIAN3_data/pg98.txt', encoding=\"utf-8\").read()\n",
    "t1 = clean_gutenberg_text(t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the entire nlp pipeline on the book. \n",
    "\n",
    "(Warning: This will take much longer than just tokenizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1=nlp(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(doc1[:500]):\n",
    "    if not re.search(\"\\s\",token.orth_):\n",
    "        print(i,token),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 100 tokens at the beginning of chapter 1 (which starts at token 368)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=368\n",
    "n=100\n",
    "d1_tokens=[token.orth_ for token in doc1]\n",
    "print(d1_tokens[i:i+n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since spaCy has tagged the tokens for us, we can now look at the lemmas corresponding to the tokens, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d1_lemmas=[token.lemma_ for token in doc1]\n",
    "print(d1_lemmas[i:i+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as an illustration, the following code achieves the same result without list comprehension\n",
    "d1_lemmas=[]\n",
    "for token in doc1:\n",
    "    d1_lemmas.append(token.lemma_)\n",
    "print(d1_lemmas[i:i+n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the corresponding *general* part-of-speech tags, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d1_pos=[token.pos_ for token in doc1]\n",
    "print(d1_pos[i:i+n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the corresponding *detailed* part-of-speech tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_pos=[token.tag_ for token in doc1]\n",
    "print(d1_pos[i:i+n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overview of the general tags can be found [here](https://spacy.io/usage/linguistic-features) and [here](https://spacy.io/api/annotation) and the detailed tags can be found [here](https://www.researchgate.net/profile/Jinho_Choi3/publication/324940566_Guidelines_for_the_CLEAR_Style_Constituent_to_Dependency_Conversion/links/5aebd3cfa6fdcc8508b6e6e8/Guidelines-for-the-CLEAR-Style-Constituent-to-Dependency-Conversion.pdf).\n",
    "\n",
    "Meanwhile, here is a small cheat sheet.\n",
    "\n",
    "![part-of-speech-tags](GIAN4_data/postags.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the correspondence between token, lemma, and part-of-speech tag in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token in doc1[i:i+n]:\n",
    "    if token.orth_.isalpha(): # remove punctuation\n",
    "        print (\"{:s} => {:s} ({:s}, {:s})\".format(token.orth_, token.lemma_, token.pos_, token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences\n",
    "\n",
    "Because spaCy has parsed the entire text, we can also look at sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentences=list(doc1.sents)\n",
    "for i, sentence in enumerate(sentences[:100]):\n",
    "    print(i,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(sentences[58], style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Running spaCy's nlp pipeline also includes [named entity recognition](https://spacy.io/usage/linguistic-features#section-named-entities)\n",
    "\n",
    "Let's look at the first 20 named entities in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([ent.orth_ for ent in doc1.ents[:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same thing, but now with named entity labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(ent.orth_, ent.label_) for ent in doc1.ents[:100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc1[:500], style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now make a frequency dictionary of the persons and geopolitical entities in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persons\n",
    "doc1_person=Counter([(ent.orth_, ent.label_) for ent in doc1.ents if ent.label_==\"PERSON\"])\n",
    "# geo-political entities\n",
    "doc1_gpe=Counter([(ent.orth_, ent.label_) for ent in doc1.ents if ent.label_==\"GPE\"])\n",
    "# dates\n",
    "doc1_dates=Counter([(ent.orth_, ent.label_) for ent in doc1.ents if ent.label_==\"DATE\"])\n",
    "# works of art\n",
    "doc1_woa=Counter([(ent.orth_, ent.label_) for ent in doc1.ents if ent.label_==\"WORK_OF_ART\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lets us answer who are the most common persons and ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_person.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... what are the most common places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_gpe.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Insights from plotting text data \n",
    "\n",
    "As an example of how plotting data can give us insight in the text, we will use the named entity recogntion data to track different persons and locations throughout the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a list of the 10 most common persons in the book ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_tracklist=[e for e, f in doc1_person.most_common(10)]\n",
    "person_tracklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and track them over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_trackdict=freq_over_time(doc1, person_tracklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the evolution of the tracked entities throughout the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "# plt.plot(person_trackdict[('Lorry', 'PERSON')], label=\"Lorry\")\n",
    "# plt.plot(person_trackdict[('Cruncher', 'PERSON')], label=\"Cruncher\")\n",
    "plt.plot(person_trackdict[('Manette', 'PERSON')], label=\"Manette\")\n",
    "plt.plot(person_trackdict[('Miss Pross', 'PERSON')], label=\"Miss Pross\")\n",
    "# plt.plot(person_trackdict[('Jerry', 'PERSON')], label=\"Jerry\")\n",
    "plt.plot(person_trackdict[('Lucie', 'PERSON')], label=\"Lucie\")\n",
    "plt.ylabel(\"cumulative frequency\")\n",
    "plt.xlabel(\"occurence in book\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpe_tracklist=[e for e, f in doc1_gpe.most_common(10)]\n",
    "gpe_trackdict=freq_over_time(doc1, gpe_tracklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpe_tracklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(gpe_trackdict[('Paris', 'GPE')], label=\"Paris\")\n",
    "# plt.plot(gpe_trackdict[('France', 'GPE')], label=\"France\")\n",
    "plt.plot(gpe_trackdict[('London', 'GPE')], label=\"London\")\n",
    "# plt.plot(gpe_trackdict[('England', 'GPE')], label=\"England\")\n",
    "plt.ylabel(\"cumulative frequency\")\n",
    "plt.xlabel(\"book progression (entities)\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(gpe_trackdict[('Paris', 'GPE')], label=\"Paris\")\n",
    "plt.plot(person_trackdict[('Miss Pross', 'PERSON')], label=\"Miss Pross\")\n",
    "plt.plot(gpe_trackdict[('London', 'GPE')], label=\"London\")\n",
    "plt.plot(person_trackdict[('Lucie', 'PERSON')], label=\"Lucie\")\n",
    "plt.ylabel(\"cumulative frequency\")\n",
    "plt.xlabel(\"book progression (entities)\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining data from parsing and named entity recognition\n",
    "\n",
    "To conclude, we combine the output of the NLP parsing with the NLP named entity recognition to see what an entity is doing during the text ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_entity=\"Paris\"\n",
    "for entity in doc1.ents: \n",
    "    if entity.orth_==track_entity:\n",
    "        token=entity[0]\n",
    "        if token.dep_==\"dobj\":\n",
    "            print(\"+ {:s}\".format(' '.join([token.orth_ for token in token.head.subtree])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to find out when two entities occur together in a sentence ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_set=set([\"France\", \"England\"],)\n",
    "for sentence in doc1.sents:\n",
    "    subdoc=nlp(sentence.orth_, )\n",
    "    entities=set([ent.orth_ for ent in subdoc.ents])\n",
    "    if len(track_set.intersection(entities))==len(track_set):\n",
    "        print('+',' '.join([token.orth_ for token in sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
