{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIAN 6: Classifying documents\n",
    "\n",
    "This notebook illustrates some basic machine learning techniques for supervised text classification. It also shows you how to evaluate classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we introduce a new package called [scikit-learn](http://scikit-learn.org), abbreviated as *sklearn*. This package contains a wealth of machine learning techniques that can be applied in text mining. It should have been installed automatically with [Anaconda](https://anaconda.org/). If it is not installed yet, you can find it under scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised classification\n",
    "\n",
    "In text mining, a supervised classification task requires:\n",
    "\n",
    "+ A collection of text-containing elements. These elements can be books, twitter messages, emails, etc.;\n",
    "\n",
    "+ For each element, a label, usually provided by a human rater at some point.\n",
    "\n",
    "The goal of supervised classification is to learn how labels can be predicted by characteristics of the text –also called features– and to be able to assign a label to a new element based on these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam and ham\n",
    "\n",
    "A typical text classification task is spam detection.\n",
    "\n",
    "Spam detection requires a collection of messages that a human has labeled to be:\n",
    "\n",
    "+ unwanted (SPAM)\n",
    "+ wanted (HAM)\n",
    "\n",
    "As an example we will use a collection of SMS messages that have been labeled SPAM or HAM.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"GIAN6_data/SMSSpamCollection\", encoding=\"utf-8\")\n",
    "messages=[]\n",
    "labels=[]\n",
    "for line in f:\n",
    "    label, message = line.strip().split(\"\\t\")\n",
    "    messages.append(message)\n",
    "    labels.append(label.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(labels[i], \":\", messages[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the text for classification (feature extraction)\n",
    "\n",
    "Our next step is to transform the texts into something that is useful for a classifier. This is also called feature extraction.\n",
    "\n",
    "In this case, we will transform the text into a \"bag of words\". This means that we extract the word types from the document and \"throw them together in a bag\". The bag contains all of the different words in the text, but the words are unordered.\n",
    "\n",
    "###  Vectorizing\n",
    "As most machine learning methods expect numerical data, we need to transform the *bag of words* to a set of numbers.\n",
    "\n",
    "Let's say we have three documents:\n",
    "\n",
    "| Document | Content                               |\n",
    "|----------|---------------------------------------|\n",
    "| 1        | I've been to Hollywood                |\n",
    "| 2        | I've been to Redwood                  |\n",
    "| 3        | I've been a miner for a heart of gold |\n",
    "\n",
    "\n",
    "After tokenizing, we can transform each document in a numeric vector (a row of numbers) by using all the possible words as columns.\n",
    "\n",
    "We obtain a matrix with words as columns and document as rows.\n",
    "\n",
    "| i | 've | been | to | hollywood | redwood | a | miner | for | heart | of | gold |\n",
    "|---|-----|------|----|-----------|---------|---|-------|-----|-------|----|------|\n",
    "| 1 | 1   | 1    | 1  | 1         | 0       | 0 | 0     | 0   | 0     | 0  | 0    |\n",
    "| 1 | 1   | 1    | 1  | 0         | 1       | 0 | 0     | 0   | 0     | 0  | 0    |\n",
    "| 1 | 1   | 1    | 0  | 0         | 0       | 1 | 1     | 1   | 1     | 1  | 1    |\n",
    "\n",
    "For clarity's sake we have put the words in the order they occur in the documents, but the actual order of the columns is irrelevant.\n",
    "\n",
    "In the example above, the cells in each row indicate whether the word in the column is present (1) or absent (0) in the document, but the cells may also include:\n",
    "\n",
    "+ The frequency of the word in the document\n",
    "+ The ${TF} \\times {IDF}$ of the word\n",
    "+ ... any other useful numerical measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use spaCy's tokenizer because it is much better than the one included with sklearn\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load(disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "\n",
    "def my_tokenizer(text):\n",
    "    return[token.lower_ for token in nlp(text)]\n",
    "\n",
    "vectorizer=CountVectorizer(tokenizer=my_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\"I've been to Hollywood\",               \n",
    "\"I've been to Redwood\",\n",
    "\"I've been a miner for a heart of gold\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the corpus into a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_m=vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the column labels (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the matrix - as you can see it contains the counts for each word in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_m.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the naive Bayes classifier\n",
    "\n",
    "We will know try to classify the SMS messages using a naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "cl_mnb=MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification requires that we divide the messages in two sets: \n",
    "+ The set that we train on (training set)\n",
    "+ The set that we test the classifier on (test set)\n",
    "\n",
    "Sklearn includes a useful function to do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split the SMS messages and labels into a training set and test set.\n",
    "\n",
    "We will use 70% of the data for training and 30% of the data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_train, messages_test, labels_train, labels_test = \\\n",
    "train_test_split(messages, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go on, let's s not forget to vectorize the messages in the spam database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_v=vectorizer.fit(messages)\n",
    "messages_v_train=vectorizer.transform(messages_train)\n",
    "messages_v_test=vectorizer.transform(messages_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the naive Bayes classifier on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_mnb.fit(messages_v_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see how well it scores on the test set! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_score=cl_mnb.score(messages_v_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a score of 1 means that all SPAM messages in the test set were classified correctly)\n",
    "\n",
    "We can also print more detailed reports, including precision, recall, and f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plabels_test=cl_mnb.predict(messages_v_test)\n",
    "print(classification_report(labels_test, plabels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these measures are nicely explained [here](https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, we can use some python to look at which messages were \n",
    "\n",
    "\n",
    "|    X        | Predicted SPAM      | Predicted HAM       |\n",
    "|-------------|---------------------|---------------------|\n",
    "| Actual SPAM | True Positive (TP)  | False Negative (FN) |\n",
    "| Actual HAM  | False Positive (FP) | True Negative (TN)  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_dict(messages, xlabels, ylabels, ref):\n",
    "    cd=defaultdict(list)\n",
    "    n=len(messages)\n",
    "    for i in range(n):\n",
    "        if xlabels[i]==ylabels[i]:\n",
    "            if xlabels[i]==ref:\n",
    "                mtype=\"TP\"\n",
    "            else:\n",
    "                mtype=\"TN\"\n",
    "        elif xlabels[i]!=ylabels[i]:\n",
    "            if xlabels[i]==ref:\n",
    "                mtype=\"FN\"\n",
    "            else:\n",
    "                mtype=\"FP\"\n",
    "        cd[mtype].append(messages[i])\n",
    "    return(cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_mnb=confusion_dict(messages_test, labels_test, plabels_test, \"SPAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_mnb[\"TN\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a k-nearest neighbors classifier\n",
    "\n",
    "Since there are many classification algorithms to choose from, lets try another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "cl_knb=KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_knb.fit(messages_v_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_knb.score(messages_v_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the nearest neighbors classifier bases its decision on 5 neighbors. Let's see what happens if we change this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knb_scores=[]\n",
    "for k in range(1, 16):\n",
    "    cl_knb=KNeighborsClassifier(n_neighbors=k)\n",
    "    cl_knb.fit(messages_v_train, labels_train)\n",
    "    knb_score=cl_knb.score(messages_v_test, labels_test)\n",
    "    knb_scores.append(knb_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,16), knb_scores)\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would happen if we used a matrix with ${TF} \\times {IDF}$ values instead of frequency values? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer_b=TfidfVectorizer(tokenizer=my_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_vb=vectorizer_b.fit(messages)\n",
    "messages_vb_train=vectorizer_b.transform(messages_train)\n",
    "messages_vb_test=vectorizer_b.transform(messages_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knb_scores_b=[]\n",
    "for k in range(1, 16):\n",
    "    cl_knb=KNeighborsClassifier(n_neighbors=k)\n",
    "    cl_knb.fit(messages_vb_train, labels_train)\n",
    "    knb_score=cl_knb.score(messages_vb_test, labels_test)\n",
    "    knb_scores_b.append(knb_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,16), knb_scores, label=\"Frequency\")\n",
    "plt.plot(range(1,16), knb_scores_b, label=r\"${TF} \\times {IDF}$\")\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
